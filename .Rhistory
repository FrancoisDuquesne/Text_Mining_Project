# Perform SVD. Specifically, reduce dimensionality down to 300 columns
# for our latent semantic analysis (LSA).
irlba <- irlba(t(tokens.tfidf), nv = 4, maxit = 60)
if (!require("easyPubMed")) install.packages("easyPubMed")
library(XML)
library(easyPubMed)
library(ggplot2)
############### PART 1: extraction des textes
##### Option 1: 698 documents via une requete
Querry_String <- "mouse"
Ids <- get_pubmed_ids(Querry_String)
# get_pubmed_ids() decompose les characteristiques de Querry_String en nombre de char et mot clef (ex: AND) et compose une querry comprehensible pour fetch_pubmed_data().
papers <- fetch_pubmed_data(Ids)
# papers est une structure type html qui contien tt les info pour tt les documents (ici +-700 doc) qui ont etes recueillis par fetch_pubmed_data()
print(papers)
if (!require("easyPubMed")) install.packages("easyPubMed")
library(XML)
library(easyPubMed)
library(ggplot2)
##### Option 1: 698 documents via une requete
Querry_String <- "mouse"
Ids <- get_pubmed_ids(Querry_String)
# get_pubmed_ids() decompose les characteristiques de Querry_String en nombre de char et mot clef (ex: AND) et compose une querry comprehensible pour fetch_pubmed_data().
papers <- fetch_pubmed_data(Ids)
# papers est une structure type html qui contien tt les info pour tt les documents (ici +-700 doc) qui ont etes recueillis par fetch_pubmed_data()
print(papers)
Abstract <- unlist(xpathApply(papers, "//AbstractText", saveXML))
head(Abstract)
# position des "<AbstractText>"
Abstract_pos <- regexpr('<AbstractText>.*<\\/AbstractText>', Abstract)
# on enleve le <AbstractText> au debut de l' Abstract.
Abstract <- substr(Abstract, Abstract_pos + 14, Abstract_pos + attributes(Abstract_pos)$match.length - 16)
head(Abstract)
# Approche Bag of words:
NbrDoc<-10
raw <- Abstract[1:NbrDoc]
print(raw)
library(quanteda)
library(quanteda)
# Tokenize
tokens <- tokens(raw, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = TRUE)
# minimize capital letters
tokens <- tokens_tolower(tokens)
# stopwords
stop<-stopwords()
new_stopwords<-append(stop,c("fig.","eq.","abstracttext"))
tokens <- tokens_select(tokens, new_stopwords, selection = "remove")
# stem
tokens <- tokens_wordstem(tokens, language = "english")
# Tokenize
tokens <- tokens(raw, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = TRUE)
# minimize capital letters
tokens <- tokens_tolower(tokens)
# stopwords
stop<-stopwords()
new_stopwords<-append(stop,c("fig.","eq.","abstracttext"))
tokens <- tokens_select(tokens, new_stopwords, selection = "remove")
# Create our first bag-of-words model.
tokens.dfm <- dfm(tokens, tolower = FALSE)
# Transform to a matrix and inspect.
tokens.matrix <- as.matrix(tokens.dfm)
View(tokens.matrix[1:NbrDoc, 1:100])
dim(tokens.matrix)
tokens.matrix
# Tokenize
tokens <- tokens(raw, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = TRUE)
tokens
Abstract
head(Abstract)
head(Abstracts)
##### Option 1: 698 documents via une requete
Querry_String <- "mouse"
Ids <- get_pubmed_ids(Querry_String)
# get_pubmed_ids() decompose les characteristiques de Querry_String en nombre de char et mot clef (ex: AND) et compose une querry comprehensible pour fetch_pubmed_data().
papers <- fetch_pubmed_data(Ids)
# papers est une structure type html qui contien tt les info pour tt les documents (ici +-700 doc) qui ont etes recueillis par fetch_pubmed_data()
print(papers)
Abstract <- unlist(xpathApply(papers, "//AbstractText", saveXML))
head(Abstract)
# position des "<AbstractText>"
Abstract_pos <- regexpr('<AbstractText>.*<\\/AbstractText>', Abstract)
# on enleve le <AbstractText> au debut de l' Abstract.
Abstract <- substr(Abstract, Abstract_pos + 14, Abstract_pos + attributes(Abstract_pos)$match.length - 16)
head(Abstract)
if (!require("easyPubMed")) install.packages("easyPubMed")
library(XML)
library(easyPubMed)
library(ggplot2)
##### Option 1: 698 documents via une requete
Querry_String <- "mouse"
Ids <- get_pubmed_ids(Querry_String)
# get_pubmed_ids() decompose les characteristiques de Querry_String en nombre de char et mot clef (ex: AND) et compose une querry comprehensible pour fetch_pubmed_data().
papers <- fetch_pubmed_data(Ids)
# papers est une structure type html qui contien tt les info pour tt les documents (ici +-700 doc) qui ont etes recueillis par fetch_pubmed_data()
print(papers)
Abstract <- unlist(xpathApply(papers, "//AbstractText", saveXML))
head(Abstract)
# position des "<AbstractText>"
Abstract_pos <- regexpr('<AbstractText>.*<\\/AbstractText>', Abstract)
# on enleve le <AbstractText> au debut de l' Abstract.
Abstract <- substr(Abstract, Abstract_pos + 14, Abstract_pos + attributes(Abstract_pos)$match.length - 16)
head(Abstract)
Abstract <- unlist(xpathApply(papers, "//AbstractText", saveXML))
head(Abstract)
##### Option 1: 698 documents via une requete
Querry_String <- "soap"
Ids <- get_pubmed_ids(Querry_String)
# get_pubmed_ids() decompose les characteristiques de Querry_String en nombre de char et mot clef (ex: AND) et compose une querry comprehensible pour fetch_pubmed_data().
papers <- fetch_pubmed_data(Ids)
Abstract <- unlist(xpathApply(papers, "//AbstractText", saveXML))
head(Abstract)
# Approche Bag of words:
NbrDoc<-10
raw <- Abstract[1:NbrDoc]
print(raw)
# Tokenize
tokens <- tokens(raw, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = TRUE)
# minimize capital letters
tokens <- tokens_tolower(tokens)
# stopwords
stop<-stopwords()
new_stopwords<-append(stop,c("fig.","eq.","abstracttext"))
tokens <- tokens_select(tokens, new_stopwords, selection = "remove")
# Create our first bag-of-words model.
tokens.dfm <- dfm(tokens, tolower = FALSE)
##### Option 1: 698 documents via une requete
Querry_String <- "mouse"
Ids <- get_pubmed_ids(Querry_String)
# get_pubmed_ids() decompose les characteristiques de Querry_String en nombre de char et mot clef (ex: AND) et compose une querry comprehensible pour fetch_pubmed_data().
papers <- fetch_pubmed_data(Ids)
# papers est une structure type html qui contien tt les info pour tt les documents (ici +-700 doc) qui ont etes recueillis par fetch_pubmed_data()
print(papers)
Abstract <- unlist(xpathApply(papers, "//AbstractText", saveXML))
head(Abstract)
# position des "<AbstractText>"
Abstract_pos <- regexpr('<AbstractText>.*<\\/AbstractText>', Abstract)
# Approche Bag of words:
NbrDoc<-10
raw <- Abstract[1:NbrDoc]
print(raw)
library(quanteda)
# Tokenize
tokens <- tokens(raw, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = TRUE)
# minimize capital letters
tokens <- tokens_tolower(tokens)
# stopwords
stop<-stopwords()
new_stopwords<-append(stop,c("fig.","eq.","abstracttext"))
tokens <- tokens_select(tokens, new_stopwords, selection = "remove")
# Create our first bag-of-words model.
tokens.dfm <- dfm(tokens, tolower = FALSE)
# Transform to a matrix and inspect.
tokens.matrix <- as.matrix(tokens.dfm)
View(tokens.matrix[1:NbrDoc, 1:100])
dim(tokens.matrix)
# Tokenfrequence
# In corpus
freq <- sort(colSums(tokens.matrix), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
# In specific document
Doc<-5
freqInDoc <- sort(tokens.matrix[Doc,], decreasing=TRUE)
wfindoc <- data.frame(word=names(freqInDoc), freq=freqInDoc)
# plot word frequence
pl <- ggplot(subset(wf, freq > 1) ,aes(word, freq))
# plot word frequence
pl <- ggplot(subset(wf, freq > 1) ,aes(word, freq))
pl <- ggplot(subset(wfindoc, freq > 1) ,aes(word, freq))
pl <- pl + geom_bar(stat="identity", fill="darkred", colour="white")
pl + theme(axis.text.x=element_text(angle=90, hjust=1)) + ggtitle("Uni-Gram Frequency")
# plot word frequence
pl <- ggplot(subset(wf, freq > 1) ,aes(word, freq))
# pl <- ggplot(subset(wfindoc, freq > 1) ,aes(word, freq))
pl <- pl + geom_bar(stat="identity", fill="darkred", colour="white")
pl + theme(axis.text.x=element_text(angle=90, hjust=1)) + ggtitle("Uni-Gram Frequency")
# Word Cloud
library(wordcloud)
set.seed(100)
wordcloud(names(freq), freq, min.freq=2, colors=brewer.pal(6, "Dark2"))
# Our function for calculating relative term frequency (TF)
term.frequency <- function(row) {
row / sum(row)
}
# Our function for calculating inverse document frequency (IDF)
inverse.doc.freq <- function(col) {
corpus.size <- length(col)
doc.count <- length(which(col > 0))
log10(corpus.size / doc.count)
}
# Our function for calculating TF-IDF.
tf.idf <- function(x, idf) {
x * idf
}
# First step, normalize all documents via TF.
tokens.df <- apply(tokens.matrix, 1, term.frequency)
dim(tokens.df)
View(tokens.df[1:100, 1:NbrDoc])
# Second step, calculate the IDF vector that we will use - both
tokens.idf <- apply(tokens.matrix, 2, inverse.doc.freq)
str(tokens.idf)
# Lastly, calculate TF-IDF for our training corpus.
tokens.tfidf <-  apply(tokens.df, 2, tf.idf, idf = tokens.idf)
dim(tokens.tfidf)
View(tokens.tfidf[1:25, 1:NbrDoc])
# Transpose the matrix
tokens.tfidf <- t(tokens.tfidf)
dim(tokens.tfidf)
View(tokens.tfidf[1:100, 1:NbrDoc])
# Transpose the matrix
tokens.tfidf <- t(tokens.tfidf)
dim(tokens.tfidf)
View(tokens.tfidf[1:100, 1:NbrDoc])
# Lastly, calculate TF-IDF for our training corpus.
tokens.tfidf <-  apply(tokens.df, 2, tf.idf, idf = tokens.idf)
dim(tokens.tfidf)
View(tokens.tfidf[1:25, 1:NbrDoc])
# Transpose the matrix
tokens.tfidf <- t(tokens.tfidf)
dim(tokens.tfidf)
View(tokens.tfidf[1:100, 1:NbrDoc])
View(tokens.tfidf[1:NbrDoc, 1:25])
# Make a clean data frame.
tokens.tfidf.df <- data.frame(tokens.tfidf)
names(tokens.tfidf.df) <- make.names(names(tokens.tfidf.df))
library(irlba)
library(irlba)
# Perform SVD. Specifically, reduce dimensionality down to 300 columns
# for our latent semantic analysis (LSA).
irlba <- irlba(t(tokens.tfidf), nv = 4, maxit = 60)
# Take a look at the new feature data up close.
View(irlba$v)
tokens.tfidf.df
View(tokens.tfidf[1:NbrDoc, 1:25])
